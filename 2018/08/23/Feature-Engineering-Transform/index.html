<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    

    <title>
      Feature Engineering (Transform) | 王鸣辉的博客
    </title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    
      <meta name="author" content="Minghui Wang">
    
    

    <meta name="description" content="近期推荐模型也做了一些，小伙伴们也尝试了各种论文的模型，诸如deepFM，NCF，DCN等等，但效果都没有明显的提升，于是也想着是不是从数据集特征入手，重新梳理遍特征工程相关的东西，同时我也在思考，到底模型接受什么样的特征是好的，尤其是对于连续数值型特征，本文算是我对特征工程部分方面记录与总结。">
<meta name="keywords" content="Feature Engineering,Transform">
<meta property="og:type" content="article">
<meta property="og:title" content="Feature Engineering (Transform) | 王鸣辉的博客">
<meta property="og:url" content="http://yoursite.com/2018/08/23/Feature-Engineering-Transform/index.html">
<meta property="og:site_name" content="王鸣辉的博客">
<meta property="og:description" content="近期推荐模型也做了一些，小伙伴们也尝试了各种论文的模型，诸如deepFM，NCF，DCN等等，但效果都没有明显的提升，于是也想着是不是从数据集特征入手，重新梳理遍特征工程相关的东西，同时我也在思考，到底模型接受什么样的特征是好的，尤其是对于连续数值型特征，本文算是我对特征工程部分方面记录与总结。">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/images/ml_pipeline.png">
<meta property="og:image" content="http://yoursite.com/images/box_cox.png">
<meta property="og:image" content="http://yoursite.com/images/transform4.png">
<meta property="og:updated_time" content="2019-05-26T16:52:16.687Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Feature Engineering (Transform) | 王鸣辉的博客">
<meta name="twitter:description" content="近期推荐模型也做了一些，小伙伴们也尝试了各种论文的模型，诸如deepFM，NCF，DCN等等，但效果都没有明显的提升，于是也想着是不是从数据集特征入手，重新梳理遍特征工程相关的东西，同时我也在思考，到底模型接受什么样的特征是好的，尤其是对于连续数值型特征，本文算是我对特征工程部分方面记录与总结。">
<meta name="twitter:image" content="http://yoursite.com/images/ml_pipeline.png">

    
    
      <link rel="icon" type="image/x-icon" href="/favicon.png">
    
    <link rel="stylesheet" href="/css/uno.css">
    <link rel="stylesheet" href="/css/highlight.css">
    <link rel="stylesheet" href="/css/archive.css">
    <link rel="stylesheet" href="/css/china-social-icon.css">

</head>
<body>

    <span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
    </span>

    

<header class="panel-cover panel-cover--collapsed">


  <div class="panel-main">


    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        

        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage">王鸣辉的博客</a></h1>
        <hr class="panel-cover__divider">

        
        <p class="panel-cover__description">
          Wang Minghui&#39;s Personal Blog
        </p>
        <hr class="panel-cover__divider panel-cover__divider--secondary">
        

        <div class="navigation-wrapper">

          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">

              
                
                <li class="navigation__item"><a href="/#blog" title class="blog-button">首页</a></li>
              
                
                <li class="navigation__item"><a href="/about" title class>关于</a></li>
              
                
                <li class="navigation__item"><a href="/archive" title class>归档</a></li>
              

            </ul>
          </nav>

          <!-- ----------------------------
To add a new social icon simply duplicate one of the list items from below
and change the class in the <i> tag to match the desired social network
and then add your link to the <a>. Here is a full list of social network
classes that you can use:

    icon-social-500px
    icon-social-behance
    icon-social-delicious
    icon-social-designer-news
    icon-social-deviant-art
    icon-social-digg
    icon-social-dribbble
    icon-social-facebook
    icon-social-flickr
    icon-social-forrst
    icon-social-foursquare
    icon-social-github
    icon-social-google-plus
    icon-social-hi5
    icon-social-instagram
    icon-social-lastfm
    icon-social-linkedin
    icon-social-medium
    icon-social-myspace
    icon-social-path
    icon-social-pinterest
    icon-social-rdio
    icon-social-reddit
    icon-social-skype
    icon-social-spotify
    icon-social-stack-overflow
    icon-social-steam
    icon-social-stumbleupon
    icon-social-treehouse
    icon-social-tumblr
    icon-social-twitter
    icon-social-vimeo
    icon-social-xbox
    icon-social-yelp
    icon-social-youtube
    icon-social-zerply
    icon-mail

-------------------------------->

<!-- add social info here -->



<nav class="cover-navigation navigation--social">
  <ul class="navigation">

    
      <!-- Github -->
      <li class="navigation__item">
        <a href="https://github.com/wd1900" title="Huno on GitHub">
          <i class="icon icon-social-github"></i>
          <span class="label">GitHub</span>
        </a>
      </li>
    

    <!-- China social icon -->


      <!-- <li class="navigation__item">
        <a href="https://github.com/wd1900" title="">
          <i class='icon cs-icon-douban'></i>
          <span class="label">Douban</span>
        </a>
      </li> -->

      <li class="navigation__item">
        <a href="http://weibo.com/p/1005051882909995/" title>
          <i class="icon cs-icon-weibo"></i>
          <span class="label">Weibo</span>
        </a>
      </li>




  </ul>
</nav>




        </div>

      </div>

    </div>

    <div class="panel-cover--overlay"></div>
  </div>
</header>


    <div class="content-wrapper">
        <div class="content-wrapper__inner entry">
            

<article class="post-container post-container--single">

  <header class="post-header">
    
    <h1 class="post-title">Feature Engineering (Transform)</h1>

    

    <div class="post-meta">
      <time datetime="2018-08-23" class="post-meta__date date">2018-08-23</time> 

      <span class="post-meta__tags tags">

          

          
             &#8226; 标签:
            <font class="tags">
              <a class="tags-link" href="/tags/Feature-Engineering/">Feature Engineering</a>, <a class="tags-link" href="/tags/Transform/">Transform</a>
            </font>
          

      </span>
    </div>
    
    

  </header>

  <section id="post-content" class="article-content post">
    <p>近期推荐模型也做了一些，小伙伴们也尝试了各种论文的模型，诸如deepFM，NCF，DCN等等，但效果都没有明显的提升，于是也想着是不是从数据集特征入手，重新梳理遍特征工程相关的东西，同时我也在思考，到底模型接受什么样的特征是好的，尤其是对于连续数值型特征，本文算是我对特征工程部分方面记录与总结。</p>
<a id="more"></a>
<h1 id="Feature-Engineering"><a href="#Feature-Engineering" class="headerlink" title="Feature Engineering"></a>Feature Engineering</h1><p>特征工程是我们在处理机器学习问题首先要面对的问题，无论是传统的机器学习还是深度学习，我们都需要将特征处理成一个合适的形式输入到训练模型中,这其实也是我们将先验的知识带入的过程，取得更有效更直观的数据，省去模型对于一部分数据特征的学习。有一句话受到了广泛的认可，就是数据和特征决定了机器学习的上限,而算法只是尽可能逼近这个上限。</p>
<blockquote>
<p>Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.</p>
<pre><code>— Dr. Jason Brownlee
</code></pre></blockquote>
<p><img src="/images/ml_pipeline.png" alt="standard machine learning pipeline"></p>
<p>所以数据到底该怎么处理，处理到什么样是一个好的特征呢，这个我们后续会一一讨论（虽然我也理解不透，哈哈😆）</p>
<p>Features一般可以分为两大类，Raw features 和 Derived features.Derived features一般就是通过特征工程产生的，比如说数据集里只有用户的出生日期，那我们可以根据生日得到年龄这一Derived feature,这也就是最基本的特征工程。</p>
<p>原始的数据集中会有各式各样的特征，我们先聊下最常见的特征，基本上就是连续数值(numeric)特征和泛类别(categorical)特征（像文字这种我也算入在内,当然文字还是比较特别的，早先是n-gram这种，现在都是类似word2vec的玩法，推荐里边也有item2vec这种弄法，不过这次不聊这块）</p>
<h2 id="Numeric-Data"><a href="#Numeric-Data" class="headerlink" title="Numeric Data"></a>Numeric Data</h2><p>数值特征可以说是最直观的特征了，许多数据集中都会看到数值特征，房价中的房屋面积，人均收入等等。数值特征的优势是一般其都可以直接作为输入进行数学运算。不过我们的目的显然不是只是为了让数据能变为数值参与计算就行，而是为了让模型能根据特征去更好的学习目标。所以，数值特征也需要对其进行进一步的特征工程。<br>要想做好特征工程，我们首先应该了解数据。一般要先看一下数据分布，均值，标准差和分位数等数据。</p>
<h3 id="1-Binarization"><a href="#1-Binarization" class="headerlink" title="1.Binarization"></a>1.Binarization</h3><p>有一些数据中次数可能不重要，比如说用户看电影，看了一遍和两遍在某些用户喜好场景下可以忽略，那么其实就是只要次数&gt;= 1那么就转化为1，没看过转化为0</p>
<h3 id="2-Rounding"><a href="#2-Rounding" class="headerlink" title="2.Rounding"></a>2.Rounding</h3><p>有一些数据，比如百分比，在一些情况下精度可能不是那么重要，其实就可以约到整数位 如99.34% =&gt; 99%, 其可以就此作为数值输入，也可以转换为分类特征</p>
<h3 id="3-Interactions"><a href="#3-Interactions" class="headerlink" title="3.Interactions"></a>3.Interactions</h3><p>比如有特征宽高，相乘可获取到面积。但许多传统模型没有组合能力，这就需要我们手动的去做。由之前的y=wx变成y = w1<em>x1 + w2</em>x2 + w12<em>x1</em>x2 … 当然有些模型本身会自己做特征组合，</p>
<h3 id="4-Binning"><a href="#4-Binning" class="headerlink" title="4.Binning"></a>4.Binning</h3><p>分桶，最常见的就是对年龄的处理，tf中有封装的函数 <code>tf.feature_column.bucketized_column</code>，其将数据按界限分隔为几个类别然后做one-hot处理。这种属于固定式的分桶，更进一步，我们可以根据数据自适应的去划分，比如n-Quantiles ，将数据划分到n个等宽的桶中，这样就由连续数值特征转变为了类别特征，再进行训练。</p>
<p>二值化和分桶甚至于进行约数其实都相当于是将数值特征转为类别特征，然后变成one-hot形式.</p>
<h3 id="5-Transform"><a href="#5-Transform" class="headerlink" title="5.Transform"></a>5.Transform</h3><p>当特征为有意义的连续数对这种特征，一般需要进行变化</p>
<pre><code>min-max
z-score
Log
Square root
1/x
Power transform (Box–Cox transformation、Yeo–Johnson transformation)
quantile
</code></pre><p>有很多数据有很强的前后大小关系，做成离散高维的会丢失很多信息。比如用户对视频、直播的观看时长，消费这种数据，有强烈的大小意义。对这种特征，<strong>最基础的是归一化</strong>，是min-max将其转为[0,1]区间，统一转化到0-1区间会提升学习速度，这个不具体转开了，不了解的同学可以google下。<br>但是现实中如果画一个分布图，会发现观看时长这类数据的曲线非常左倾（<strong>Left Skewed</strong>）,直接min-max后会发现，大部分的特征值可能都在0.01以内。<br>我们这时候可以套入常规的wx+b中，会发现这个特征大部分值太小了，导致在整个模型中基本没起作用。<br>粗暴的处理方法是直接将少部分特别大的值砍掉，超过给定max的都归一化到1.<br>这种方案对于有些数据还是不够work，大部分数据还是会积压在一块，除非砍掉特别多的大数据，但这又会丢掉不少信息。<br>于是对于skewed数据，我们可以选用log函数进行变换处理，他可以把聚集的小数拉宽，把稀疏的大数缩短，在很多情况下一个合适的底数都能将这种倾斜数据转为类似正太分布。<br>这种方案能解决一部分问题，还有些比较少见但好像很强的方法 ，Power transform<br>包括Box–Cox transformation、Yeo–Johnson transformation<br>以Box-Cox为例<img src="/images/box_cox.png" alt><br>对原始数据进行极大似然估计，找出最大可能的lmbda值，对所有数据进行变换，将数据转换为类似正太分布的形式。</p>
<p>拿我们自己的真实数据举例子，下图展现的是原始分布，已经经过变换后的分布<br><img src="/images/transform4.png" alt><br>我们可以看到原始数据十分倾斜，绝大部分数据归一化到[0,1]区间后，其值会十分接近0，没有区分度，这样的特征也就很难在模型中起到作用<br>在线上对部分特征做了box-cox的对比，发现log通常会好一些。</p>
<p><strong>这里也就是我想要强调的地方，优先选择直观可解释的变换，比如X是单位面积的房价，取了倒数后就变成了，一元能买多少平，log，exp也是类似，可解释性要好一些。Box-Cox不是银弹，它的可解释性要差很多，它对于寻找一个对应的变换是有用的，但是其最优指并不应该盲目去使用它，可以看一下置信区间，如果log或平方根能解决，那么优先使用这两种转换方式。</strong></p>
<p><strong>不要随意使用这种强力变换，除非确定特征符合某种非线性特征，否则可能会引入更多的非线性，导致更难拟合</strong></p>
<p>这里边再补个demo代码，这是我用来测神经网络拟合log函数的，你会发现不同的网络层数、神经元数目和激活函数对应学习效果上的巨大影响。</p>
<blockquote>
<p>Fortunately, feedforward networks with hidden layers provide a universal approxi-mation framework. Speciﬁcally, theuniversal approximation theorem(Horniket al., 1989; Cybenko, 1989) states that a feedforward network with a linear outputlayer and at least one hidden layer with any “squashing” activation function (suchas the logistic sigmoid activation function) can approximate any Borel measurablefunction from one ﬁnite-dimensional space to another with any desired nonzeroamount of error, provided that the network is given enough hidden units.</p>
</blockquote>
<p>理论上ANN可以不做特征变换和选择，模型能自己处理，不过我还是觉得如果已经有先验经验的情况下，还是自己做一些比如log变换，然后归一化的会比直接归一化让模型自己去学习log这种变换方式要更好些。<br>另外，<a href="ftp://ftp.sas.com/pub/neural/FAQ2.html#A_std" target="_blank" rel="noopener">ftp://ftp.sas.com/pub/neural/FAQ2.html#A_std</a> 上说将特征归一化到[-1,1]之间会更好</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成1000个数据集, y = sum(log(x)) + random</span></span><br><span class="line">data_X = np.array([])</span><br><span class="line">data_Y = np.array([])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6000</span>):</span><br><span class="line">    <span class="comment">#data_x =  np.random.uniform(1,1000,10)</span></span><br><span class="line">    data_x = np.random.random(<span class="number">1</span>)*<span class="number">3</span> + <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">        print(data_x)</span><br><span class="line">    data_y = sum(data_x**<span class="number">3</span>)</span><br><span class="line">    <span class="comment">#data_y = sum( 2 * data_x + 4)</span></span><br><span class="line">    data_X = np.append(data_X, data_x)</span><br><span class="line">    data_Y = np.append(data_Y, data_y)</span><br><span class="line"></span><br><span class="line">data_X = data_X.reshape(<span class="number">6000</span>,<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X_train, X_valid = data_X[:<span class="number">5000</span>], data_X[<span class="number">5000</span>:]</span><br><span class="line">y_train, y_valid = data_Y[:<span class="number">5000</span>], data_Y[<span class="number">5000</span>:]</span><br><span class="line"></span><br><span class="line">n_inputs = <span class="number">1</span></span><br><span class="line">X = tf.placeholder(tf.float32, shape=(<span class="literal">None</span>, n_inputs), name=<span class="string">"X"</span>)</span><br><span class="line">y = tf.placeholder(tf.float32, shape=(<span class="literal">None</span>), name=<span class="string">"y"</span>)</span><br><span class="line"></span><br><span class="line">he_init = tf.variance_scaling_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"dnn"</span>):</span><br><span class="line">    he_inithe_init = tf.variance_scaling_initializer()</span><br><span class="line">    hidden1 = tf.layers.dense(X, <span class="number">50</span>, activation=tf.nn.elu, kernel_initializer=he_init, name=<span class="string">"hidden1"</span>)</span><br><span class="line">    hidden2 = tf.layers.dense(hidden1, <span class="number">20</span>, activation=tf.nn.elu, kernel_initializer=he_init, name=<span class="string">"hidden2"</span>)</span><br><span class="line">    <span class="comment"># hidden3 = tf.layers.dense(hidden2, 50, activation=tf.nn.relu, kernel_initializer=he_init, name="hidden3")</span></span><br><span class="line">    logits = tf.layers.dense(hidden2, <span class="number">1</span>, name=<span class="string">"outputs"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"loss"</span>):</span><br><span class="line">    loss = tf.losses.mean_squared_error(y, logits)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"train"</span>):</span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(learning_rate)</span><br><span class="line">    training_op = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line">n_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shuffle_batch</span><span class="params">(X, y, batch_size)</span>:</span></span><br><span class="line">    rnd_idx = np.random.permutation(len(X))</span><br><span class="line">    n_batches = len(X) // batch_size</span><br><span class="line">    <span class="keyword">for</span> batch_idx <span class="keyword">in</span> np.array_split(rnd_idx, n_batches):</span><br><span class="line">        X_batch, y_batch = X[batch_idx], y[batch_idx]</span><br><span class="line">        <span class="keyword">yield</span> X_batch, y_batch</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init.run()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">        <span class="keyword">for</span> X_batch, y_batch <span class="keyword">in</span> shuffle_batch(X_train, y_train, batch_size):</span><br><span class="line">            <span class="comment">#print(X_batch, y_batch)</span></span><br><span class="line">            sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            train_loss = sess.run(loss, feed_dict=&#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">            valid_loss = sess.run(loss, feed_dict=&#123;X: X_valid[:<span class="number">200</span>], y: y_valid[:<span class="number">200</span>]&#125;)</span><br><span class="line">            print(epoch, <span class="string">"Batch loss:"</span>, train_loss, <span class="string">"Validation loss:"</span>, valid_loss/<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">    new_X = np.sort(X_valid[<span class="number">200</span>:<span class="number">300</span>].reshape(<span class="number">-1</span>)).reshape(<span class="number">100</span>,<span class="number">1</span>)</span><br><span class="line">    pred = sess.run(logits, feed_dict=&#123;X:new_X&#125;)</span><br><span class="line"></span><br><span class="line">    plt.plot(np.sort(X_valid[<span class="number">200</span>:<span class="number">300</span>].reshape(<span class="number">-1</span>)), pred)</span><br><span class="line">    plt.grid(color=<span class="string">"k"</span>, linestyle=<span class="string">":"</span>)</span><br><span class="line">    plt.plot(np.sort(X_valid[<span class="number">200</span>:<span class="number">300</span>].reshape(<span class="number">-1</span>)), np.sort(y_valid[<span class="number">200</span>:<span class="number">300</span>]))</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<p>在简单说一下类别特征</p>
<h1 id="Categorical-Data"><a href="#Categorical-Data" class="headerlink" title="Categorical Data"></a>Categorical Data</h1><h2 id="Nominal-categorical"><a href="#Nominal-categorical" class="headerlink" title="Nominal categorical"></a>Nominal categorical</h2><p>直播的类别，游戏，户外，音乐，跳舞等等</p>
<h2 id="Ordinal-categorical"><a href="#Ordinal-categorical" class="headerlink" title="Ordinal categorical"></a>Ordinal categorical</h2><p>衣服的尺码，S M L XL XXL…</p>
<h3 id="One-hot-Encoding"><a href="#One-hot-Encoding" class="headerlink" title="One-hot Encoding"></a>One-hot Encoding</h3><p>100  010  001</p>
<h3 id="Dummy-Coding"><a href="#Dummy-Coding" class="headerlink" title="Dummy Coding"></a>Dummy Coding</h3><p>00  01  10</p>
<h3 id="Effect-Coding"><a href="#Effect-Coding" class="headerlink" title="Effect Coding"></a>Effect Coding</h3><p>-1-1  01  10</p>
<h3 id="Hash"><a href="#Hash" class="headerlink" title="Hash"></a>Hash</h3><h3 id="Embeding"><a href="#Embeding" class="headerlink" title="Embeding"></a>Embeding</h3><p>Dummy和One-hot其实比较接近，不过相较于One-hot是少一维的,知乎王赟的回答说的清晰些，如果你不使用regularization，那么one-hot encoding的模型会有多余的自由度。这个自由度体现在你可以把某一个分类型变量各个值对应的权重都增加某一数值，同时把另一个分类型变量各个值对应的权重都减小某一数值，而模型不变。在dummy encoding中，这些多余的自由度都被统摄到intercept里去了。这么看来，dummy encoding更好一些。<br>如果你使用regularization，那么regularization就能够处理这些多余的自由度。此时，我觉得用one-hot encoding更好，因为每个分类型变量的各个值的地位就是对等的了。<br>以线性模型举例， 分类超平面是 wx+b =0，dummy下的话 w 有唯一解，one-hot 下 w 有无穷解 (就是上边所说的一部分权重增加点，另一部分权重减少点)，这样每个变量的权重就没有解释意义了，也使得模型没有真正的预测效果。加了正则化之后，相当于约束了 w 的解空间，使得 w 相对有意义</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="ftp://ftp.sas.com/pub/neural/FAQ.html" target="_blank" rel="noopener">ftp://ftp.sas.com/pub/neural/FAQ.html</a><br><a href="ftp://ftp.sas.com/pub/neural/FAQ2.html#A_std" target="_blank" rel="noopener">ftp://ftp.sas.com/pub/neural/FAQ2.html#A_std</a><br><a href="http://www.ijcte.org/papers/288-L052.pdf" target="_blank" rel="noopener">http://www.ijcte.org/papers/288-L052.pdf</a><br><a href="https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b" target="_blank" rel="noopener">https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b</a><br><a href="https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63" target="_blank" rel="noopener">https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63</a><br><a href="https://www.zhihu.com/question/48674426/answer/112633127" target="_blank" rel="noopener">https://www.zhihu.com/question/48674426/answer/112633127</a></p>

  </section>

  

<section class="post-comments">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
      var disqus_shortname = 'wd1900'; 
      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>


  

</article>


            <footer class="footer">

    <span class="footer__copyright">&copy; 2014-2015. | 由<a href="https://hexo.io/">Hexo</a>强力驱动 | 主题<a href="https://github.com/someus/huno">Huno</a></span>
    
</footer>
        </div>
    </div>

    <!-- js files -->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/main.js"></script>
    <script src="/js/scale.fix.js"></script>


    

    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript"> 
        $(document).ready(function(){
            MathJax.Hub.Config({ 
                tex2jax: {inlineMath: [['[latex]','[/latex]'], ['\\(','\\)']]} 
            });
        });
    </script>


    

    <script src="/js/awesome-toc.min.js"></script>
    <script>
        $(document).ready(function(){
            $.awesome_toc({
                overlay: true,
                contentId: "post-content",
            });
        });
    </script>


    
    
    <!--kill ie6 -->
<!--[if IE 6]>
  <script src="//letskillie6.googlecode.com/svn/trunk/2/zh_CN.js"></script>
<![endif]-->

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>
